{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":11394,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8332}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yujansaya/gemma-7b-with-lora-prompt-recovery?scriptVersionId=165719476\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers -U\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-06T13:51:01.489115Z","iopub.execute_input":"2024-03-06T13:51:01.489711Z","iopub.status.idle":"2024-03-06T13:52:18.024037Z","shell.execute_reply.started":"2024-03-06T13:51:01.48967Z","shell.execute_reply":"2024-03-06T13:52:18.02311Z"},"_kg_hide-output":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2vvyzula\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-2vvyzula\n  Resolved https://github.com/huggingface/transformers to commit 9322576e2f49d1014fb0c00a7a7c8c34b6a5fd35\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8662453 sha256=35c4b4f922277fe66f23934ef3846f86e1fc6a8c6603d277f419ff6f823188ee\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ges0cwfb/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed transformers-4.39.0.dev0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.26.4)\nDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.42.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from accelerate import Accelerator\nimport transformers\nimport bitsandbytes\nimport torch\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2024-03-06T13:52:41.162805Z","iopub.execute_input":"2024-03-06T13:52:41.1637Z","iopub.status.idle":"2024-03-06T13:52:47.684464Z","shell.execute_reply.started":"2024-03-06T13:52:41.163667Z","shell.execute_reply":"2024-03-06T13:52:47.683729Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"accelerator = Accelerator()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T13:52:53.575672Z","iopub.execute_input":"2024-03-06T13:52:53.576142Z","iopub.status.idle":"2024-03-06T13:52:53.58688Z","shell.execute_reply.started":"2024-03-06T13:52:53.576116Z","shell.execute_reply":"2024-03-06T13:52:53.585762Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T13:52:55.404246Z","iopub.execute_input":"2024-03-06T13:52:55.40462Z","iopub.status.idle":"2024-03-06T13:52:55.410577Z","shell.execute_reply.started":"2024-03-06T13:52:55.404569Z","shell.execute_reply":"2024-03-06T13:52:55.409669Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/2\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/7b-it/2\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/gemma/transformers/7b-it/2\",\n    device_map = \"auto\",\n    trust_remote_code = True,\n    quantization_config=quantization_config,\n)\n\n# model = model.to_bettertransformer()\nmodel = accelerator.prepare(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T13:52:59.051977Z","iopub.execute_input":"2024-03-06T13:52:59.052959Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e52cfa34964261837b82201d79811b"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\n\nTEST_DF_FILE = '/kaggle/input/llm-prompt-recovery/test.csv'\nSUB_DF_FILE = '/kaggle/input/llm-prompt-recovery/sample_submission.csv'\nNROWS = 1000\n\nTRAIN_DF_FILE = '/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv'\n\ntrain_df = pd.read_csv(TRAIN_DF_FILE, nrows=NROWS)\n    \ntdf = pd.read_csv(TEST_DF_FILE, usecols=['id', 'original_text', 'rewritten_text'])\nsub = pd.read_csv(SUB_DF_FILE, usecols=['id', 'rewrite_prompt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_txt(text, length):\n    text_list = text.split()\n    \n    if len(text_list) <= length:\n        return text\n    \n    return \" \".join(text_list[:length])\n\n\ndef gen_prompt(og_text, rewritten_text):\n    \n    # Truncate the texts to first 200 words for now\n    # As we are having memory issues on Mixtral8x7b\n    og_text = truncate_txt(og_text, 150)\n    rewritten_text = truncate_txt(rewritten_text, 150)\n    \n    return f\"\"\"    \n    Original Essay:\n    \\\"\"\"{og_text}\\\"\"\"\n    \n    Rewritten Essay:\n    \\\"\"\"{rewritten_text}\\\"\"\"\n    \n    Instruction:\n    Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n    You are trying to understand how the original essay was transformed into a new version.\n    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n    Only give me the PROMPT. Start directly with the prompt, that's all I need. Output should be only line ONLY.\n    \n    Response: \n    \\\"\"\"\\\"\"\"\n    \"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nstart_time = datetime.datetime.now()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport re\n\ndevice = accelerator.device\n#tdf['id'] = sub['id'].copy()\n\npbar = tqdm(total=tdf.shape[0])\n\nit = iter(tdf.iterrows())\nidx, row = next(it, (None, None))\n\n# https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481116\nDEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n\nres = []\n\nwhile idx is not None:\n    \n    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n        res.append([row[\"id\"], DEFAULT_TEXT])\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n        continue\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n        \n    try:        \n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": gen_prompt(row[\"original_text\"], row[\"rewritten_text\"])\n            }\n        ]\n        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n        \n        with torch.no_grad():\n            encoded_output = model.generate(encoded_input, max_new_tokens=50, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n        \n        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n        decoded_output = result = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n                \n        res.append([row[\"id\"], decoded_output])\n                            \n    except Exception as e:\n        print(f\"ERROR: {e}\")\n        res.append([row[\"id\"], DEFAULT_TEXT])\n        \n    finally:\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n\n        \npbar.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model = get_peft_model(model, \n                            lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset,Dataset\n\ndata = Dataset.from_pandas(train_df)\ndata = data.map(lambda samples: tokenizer(samples[\"original_text\"]), batched=True)\ndata = data.map(lambda samples: tokenizer(samples[\"rewritten_text\"]), batched=True)\ndata = data.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"]), batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets.arrow_writer import SchemaInferenceError","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\ndef formatting_func(example):\n    text = f\"Original Essay:\\n{truncate_txt(example['original_text'][0], 150)}\\n\\nRewritten Essay:\\n{truncate_txt(example['rewritten_text'][0], 150)}\\n\\nInstruction:\\n Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.You are trying to understand how the original essay was transformed into a new version.Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.Only give me the PROMPT. Start directly with the prompt, that's all I need. Output should be only line ONLY.\\n\\nResponse: \\n{truncate_txt(example['rewrite_prompt'][0], 150)}\"\n    return [text]\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=5,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_func,\n    #max_seq_length=8192\n)\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\n\npbar = tqdm(total=tdf.shape[0])\n\nit = iter(tdf.iterrows())\nidx, row = next(it, (None, None))\n\n# https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481116\nDEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n\nres = []\n\nwhile idx is not None:\n    \n    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n        res.append([row[\"id\"], DEFAULT_TEXT])\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n        continue\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n        \n    try:        \n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": gen_prompt(row[\"original_text\"], row[\"rewritten_text\"] )\n            }\n        ]\n        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n        \n        with torch.no_grad():\n            encoded_output = model.generate(encoded_input, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n        \n        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n        decoded_output = result = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n                \n        res.append([row[\"id\"], decoded_output])\n                            \n    except Exception as e:\n        print(f\"ERROR: {e}\")\n        res.append([row[\"id\"], DEFAULT_TEXT])\n        \n    finally:\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n\n        \npbar.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(res, columns=['id', 'rewrite_prompt'])\n\n#sub.to_csv(\"sample_submission.csv\", index=False)\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
